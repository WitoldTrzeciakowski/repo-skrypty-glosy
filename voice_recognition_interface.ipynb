{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Recognition Security System\n",
    "Interface for voice-based access control using ML classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafal/Documents/itml/repo-skrypty-glosy/myenv/lib/python3.12/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.5.1+cu124\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host rafal-IdeaPad-Gaming-3-15ACH6\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mGit commit: 270ef51, branch: main\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /home/rafal/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/rafal/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2025-01-29 00:13:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafal/Documents/itml/repo-skrypty-glosy/myenv/lib/python3.12/site-packages/df/checkpoint.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latest = torch.load(latest, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from torchvision import transforms\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from resample_audio_and_clear_of_noise import re_sample_audio, is_valid_wav_file\n",
    "from torchvision.models import resnet18\n",
    "from silence_removal import process_audio_file\n",
    "from create_spectrogram import process_audio_file as specotgram_process\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "\n",
    "# Constants\n",
    "LOCATORS_SPEAKERS_LIST = [\"f1\", \"f7\", \"f8\", \"m3\", \"m6\", \"m8\"]\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_PATH = 'trained_model9.pth'\n",
    "\n",
    "# Initialize DeepFilter for noise reduction\n",
    "model_df, df_state, _ = init_df()\n",
    "\n",
    "# Transform for spectrogram processing\n",
    "spec_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8057/3958822142.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "def initialize_model():\n",
    "    \"\"\"Initialize and load the ResNet18 model\"\"\"\n",
    "    model = resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(LOCATORS_SPEAKERS_LIST) + 1)\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Initialize the classification model\n",
    "classification_model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_noise_for_file(audio_path, model, df_state):\n",
    "    \"\"\"Process and remove noise from a single audio file.\"\"\"\n",
    "    try:\n",
    "        if not is_valid_wav_file(audio_path):\n",
    "            print(f\"Skipping invalid WAV file: {audio_path}\")\n",
    "            return\n",
    "        \n",
    "        audio, _ = load_audio(audio_path, sr=df_state.sr())\n",
    "        enhanced = enhance(model, df_state, audio)\n",
    "        \n",
    "        enhanced_audio_path = audio_path.replace('.wav', '_enhanced.wav') \n",
    "        save_audio(enhanced_audio_path, enhanced, df_state.sr())\n",
    "        \n",
    "        print(f\"Processed: {audio_path}\")\n",
    "        return enhanced_audio_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "def classify_segment_from_path(spectrogram_path, model):\n",
    "    \"\"\"Classify a spectrogram image from a file path using the ResNet model.\"\"\"\n",
    "    try:\n",
    "        spec_image = Image.open(spectrogram_path).convert('RGB')\n",
    "        spec_tensor = spec_transform(spec_image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(spec_tensor)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            pred_idx = torch.argmax(probs, dim=1).item()\n",
    "            confidence = probs[0][pred_idx].item() * 100\n",
    "        \n",
    "        return pred_idx, confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during classification: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"Process an audio file through the complete pipeline: enhance, split, and classify.\"\"\"\n",
    "    try:\n",
    "        print(\"Starting audio processing...\")\n",
    "        \n",
    "        # Resample the audio\n",
    "        print(\"Resampling audio...\")\n",
    "        resampled_path = re_sample_audio(file_path)\n",
    "        \n",
    "        # Remove noise\n",
    "        print(\"Removing noise...\")\n",
    "        enhanced_path = delete_noise_for_file(resampled_path, model_df, df_state)\n",
    "        \n",
    "        # Split into segments\n",
    "        print(\"Splitting into segments...\")\n",
    "        audio_paths = process_audio_file(enhanced_path)\n",
    "        \n",
    "        # Process each segment\n",
    "        authorized_count = 0\n",
    "        total_segments = 0\n",
    "        \n",
    "        print(\"\\nAnalyzing segments:\")\n",
    "        for audio_path in audio_paths:\n",
    "            spectrogram_path = specotgram_process(audio_path, \"temp\")\n",
    "            predicted_class, confidence = classify_segment_from_path(spectrogram_path, classification_model)\n",
    "            \n",
    "            if predicted_class is not None:\n",
    "                total_segments += 1\n",
    "                speaker = LOCATORS_SPEAKERS_LIST[predicted_class] if predicted_class < 6 else \"Unauthorized\"\n",
    "                print(f\"Segment {total_segments}: {speaker} (Confidence: {confidence:.2f}%)\")\n",
    "                \n",
    "                if predicted_class < 6:\n",
    "                    authorized_count += 1\n",
    "        \n",
    "        # Make final decision\n",
    "        if total_segments > 0:\n",
    "            print(\"\\nAccess Decision:\")\n",
    "            if authorized_count > total_segments / 2:\n",
    "                print(\"✅ ACCESS GRANTED\")\n",
    "            else:\n",
    "                print(\"❌ ACCESS DENIED\")\n",
    "            print(f\"Authorized segments: {authorized_count}/{total_segments}\")\n",
    "            \n",
    "            # Play the original audio\n",
    "            display(Audio(file_path))\n",
    "        else:\n",
    "            print(\"No valid segments found for analysis\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad43ff291f544a6b78f687bac8156b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Upload and Process WAV File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214d856e562e44c0b64af5c3606c518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Button, Output\n",
    "\n",
    "output = Output()\n",
    "button = Button(description='Upload and Process WAV File')\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title='Select WAV File',\n",
    "            filetypes=[('WAV files', '*.wav')]\n",
    "        )\n",
    "        if file_path:\n",
    "            process_file(file_path)\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "display(button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
