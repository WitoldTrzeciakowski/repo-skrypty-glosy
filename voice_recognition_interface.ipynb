{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Recognition Security System\n",
    "Interface for voice-based access control using ML classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "model_df, df_state, _ = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, Output, VBox\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_path, min_segment_length=4):\n",
    "    \"\"\"Process audio through preprocessing pipeline\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=48000)\n",
    "        \n",
    "        audio_tensor = torch.from_numpy(audio).float()\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)  # Add channel dimension [1, samples]\n",
    "        \n",
    "        enhanced = enhance(model_df, df_state, audio_tensor)\n",
    "        enhanced_numpy = enhanced.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        non_silent = librosa.effects.split(enhanced_numpy, top_db=60)\n",
    "        \n",
    "        segments = []\n",
    "        for start, end in non_silent:\n",
    "            segment = enhanced_numpy[start:end]\n",
    "            if len(segment) / sr >= min_segment_length:\n",
    "                segments.append(segment)\n",
    "        \n",
    "        print(f\"Successfully processed audio with {len(segments)} segments\")\n",
    "        return segments, sr\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_audio: {str(e)}\")\n",
    "        print(f\"Audio shape: {audio.shape}\")\n",
    "        print(f\"Audio tensor shape before enhance: {audio_tensor.shape}\")\n",
    "        raise\n",
    "\n",
    "def create_spectrogram(audio, sr):\n",
    "    \"\"\"Generate spectrogram from audio\"\"\"\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "def prepare_for_model(S_db):\n",
    "    \"\"\"Convert spectrogram to model input format\"\"\"\n",
    "    try:\n",
    "        # Normalize to range [0, 1]\n",
    "        S_db_normalized = (S_db - S_db.min()) / (S_db.max() - S_db.min())\n",
    "        \n",
    "        # Resize to model requirements\n",
    "        S_db_resized = np.array(Image.fromarray(S_db_normalized).resize((224, 224)))\n",
    "        \n",
    "        # Convert to tensor and add RGB channels\n",
    "        tensor = torch.FloatTensor(S_db_resized).unsqueeze(0).repeat(3, 1, 1)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        tensor = normalize(tensor)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_for_model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path='trained_model3.pth'):\n",
    "    \"\"\"Load trained classification model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 20)\n",
    "    \n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "try:\n",
    "    model = load_model()\n",
    "    print(f\"Model loaded successfully on {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(file_path):\n",
    "    \"\"\"Process audio file and show results\"\"\"\n",
    "    try:\n",
    "        print(\"Loading and processing audio file...\")\n",
    "        segments, sr = process_audio(file_path)\n",
    "        print(f\"Found {len(segments)} valid segments\")\n",
    "        \n",
    "        authorized_count = 0\n",
    "        total_confidence = 0\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        for i, segment in enumerate(segments):\n",
    "            print(f\"\\nProcessing segment {i+1}...\")\n",
    "            try:\n",
    "                spec = create_spectrogram(segment, sr)\n",
    "                print(f\"Spectrogram shape: {spec.shape}\")\n",
    "                print(f\"Spectrogram range: [{spec.min():.2f}, {spec.max():.2f}]\")\n",
    "                \n",
    "                model_input = prepare_for_model(spec)\n",
    "                print(f\"Model input shape: {model_input.shape}\")\n",
    "                print(f\"Model input range: [{model_input.min():.2f}, {model_input.max():.2f}]\")\n",
    "                \n",
    "                model_input = model_input.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(model_input)\n",
    "                    probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                    pred_idx = torch.argmax(output).item()\n",
    "                    is_authorized = pred_idx in [0, 1, 2, 3, 4, 5]\n",
    "                    confidence = probs[0][pred_idx].item() * 100\n",
    "                    total_confidence += confidence\n",
    "                    \n",
    "                    if is_authorized:\n",
    "                        authorized_count += 1\n",
    "                    \n",
    "                    print(f\"Segment {i+1}: {'Authorized' if is_authorized else 'Unauthorized'} \"\n",
    "                          f\"(Confidence: {confidence:.2f}%)\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {i+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if len(segments) > 0:\n",
    "            final_authorized = authorized_count > len(segments) / 2\n",
    "            avg_confidence = total_confidence / len(segments)\n",
    "            \n",
    "            print(\"\\nAccess Decision:\")\n",
    "            print(\"✅ ACCESS GRANTED\" if final_authorized else \"❌ ACCESS DENIED\")\n",
    "            print(f\"Average Confidence: {avg_confidence:.2f}%\")\n",
    "            print(f\"Authorized segments: {authorized_count}/{len(segments)}\")\n",
    "        else:\n",
    "            print(\"\\n❌ ACCESS DENIED - No valid segments found\")\n",
    "        \n",
    "        display(Audio(file_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_process():\n",
    "    \"\"\"Handle file selection and processing\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title='Select WAV File',\n",
    "        filetypes=[('WAV files', '*.wav')]\n",
    "    )\n",
    "    if file_path:\n",
    "        process_uploaded_file(file_path)\n",
    "\n",
    "output = Output()\n",
    "button = Button(description='Upload and Process WAV File')\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        upload_and_process()\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "display(button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python", 
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
