{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Recognition Security System\n",
    "Interface for voice-based access control using ML classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.5.1+cu124\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host rafal-IdeaPad-Gaming-3-15ACH6\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mGit commit: 89ba0c3, branch: feature/add-audio-upload-interface\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at /home/rafal/.cache/DeepFilterNet/DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafal/Documents/itml/repo-skrypty-glosy/myenv/lib/python3.12/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-25 17:49:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/rafal/.cache/DeepFilterNet/DeepFilterNet3/checkpoints/model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-11-25 17:49:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafal/Documents/itml/repo-skrypty-glosy/myenv/lib/python3.12/site-packages/df/checkpoint.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  latest = torch.load(latest, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "\n",
    "# Initialize DeepFilter for noise reduction\n",
    "model_df, df_state, _ = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, Output, VBox\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_path, min_segment_length=4):\n",
    "    \"\"\"Process audio through preprocessing pipeline\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=48000)\n",
    "    enhanced = enhance(model_df, df_state, audio)\n",
    "    non_silent = librosa.effects.split(enhanced, top_db=60)\n",
    "    \n",
    "    segments = []\n",
    "    for start, end in non_silent:\n",
    "        segment = enhanced[start:end]\n",
    "        if len(segment) / sr >= min_segment_length:\n",
    "            segments.append(segment)\n",
    "            \n",
    "    return segments, sr\n",
    "\n",
    "def create_spectrogram(audio, sr):\n",
    "    \"\"\"Generate spectrogram from audio\"\"\"\n",
    "    spectrogram = librosa.stft(audio)\n",
    "    return librosa.amplitude_to_db(abs(spectrogram))\n",
    "\n",
    "def prepare_for_model(spectrogram):\n",
    "    \"\"\"Convert spectrogram to model input format\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "    \n",
    "    spectrogram = torch.from_numpy(spectrogram)\n",
    "    spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min())\n",
    "    spectrogram = (spectrogram * 255).type(torch.uint8)\n",
    "    return transform(spectrogram).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path='trained_model3.pth'):\n",
    "    \"\"\"Load trained classification model\"\"\"\n",
    "    model = resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 20)\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(file_path):\n",
    "    \"\"\"Process audio file and show results\"\"\"\n",
    "    try:\n",
    "        segments, sr = process_audio(file_path)\n",
    "        print(f\"Found {len(segments)} valid segments\")\n",
    "        \n",
    "        authorized_count = 0\n",
    "        for i, segment in enumerate(segments):\n",
    "            spec = create_spectrogram(segment, sr)\n",
    "            model_input = prepare_for_model(spec)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(model_input)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                is_authorized = bool(torch.argmax(output) in [0,1,2,3,4,5])\n",
    "                confidence = probs[0][torch.argmax(output)].item() * 100\n",
    "                \n",
    "                if is_authorized:\n",
    "                    authorized_count += 1\n",
    "        \n",
    "        final_authorized = authorized_count > len(segments) / 2\n",
    "        print(\"\\nAccess Decision:\")\n",
    "        print(\"✅ ACCESS GRANTED\" if final_authorized else \"❌ ACCESS DENIED\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "        \n",
    "        display(Audio(file_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446ad191ba1f47ea8f4bb82ec8adf49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Upload and Process WAV File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d812feced6e9412e81746039d80d6976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def upload_and_process():\n",
    "    \"\"\"Handle file selection and processing\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title='Select WAV File',\n",
    "        filetypes=[('WAV files', '*.wav')]\n",
    "    )\n",
    "    if file_path:\n",
    "        process_uploaded_file(file_path)\n",
    "\n",
    "# Create interface\n",
    "output = Output()\n",
    "button = Button(description='Upload and Process WAV File')\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        upload_and_process()\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "display(button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
